# -*- coding: utf-8 -*-
"""ChordTrainer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fp8GN-NwJxMlwKPjxX5OOeq_UmVkGW-v
"""

import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
# !pip install scikit-learn

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import confusion_matrix
# from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score

from IPython.display import Audio
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from numpy import random
from scipy import signal

from scipy.fft import fft
from scipy.fftpack import fft, dct
import pandas as pd
import numpy as np
import librosa
import joblib
import copy
import os
import math
import sys


from random import shuffle
from math import floor
np.random.seed(42)
!pip install -Uqq ipdb
import ipdb

from google.colab import drive
drive.mount('/content/drive/',force_remount = True)
# !unzip drive/My\ Drive/chord/trying8k.zip
# !unzip drive/My\ Drive/chord/trying8kv5.zip
# !unzip drive/My\ Drive/chord/trying8kv6.zip
!unzip drive/My\ Drive/chord/trying8kv10.zip

"""Extract MFCC feature from .wav files

## Part 1 : Functions
"""

def normalize_signal_d(tone_arr, mean_val, std_val):
    tone_arr_norm = (tone_arr - mean_val)/std_val
    return tone_arr_norm

def normalize_signal(sig):
    # mean_val = -1.1961
    # std_val = 3442.3758
    mean_val = np.mean(sig)
    std_val = np.std(sig)
    sig = (sig - mean_val)/( std_val)
    return sig


tones = ["Af",
        "An",
        "Bf",
        "Bn",
        "Cn",
        "Df",
        "Dn",
        "Ef",
        "En",
        "Fn",
        "Gf",
        "Gn"]

tones = np.zeros(12)
for i in range(len(tones)):
  tones[i] = i + 1

print(tones)



def get_tones(tone):
    name = "./trying8kv10/"+str(int(tones[tone]))
    all_files = os.listdir(os.path.abspath(name))
    data_files = list(filter(lambda file: file.endswith('.txt'), all_files))
    return data_files




def compute_power(sig):
    acc = 0
    for val in sig:
        acc += val * val
    return np.sqrt(acc)/len(sig)


def remove_silence(sig_in, tone=1):
    start = 0
    end = len(sig_in)
    sig_in = np.array(sig_in)
    n = len(sig_in)

    max_val = 0
    max_idx  = 0
    for i in np.arange(0, n-64, 1):
        slide_start = i
        slide_end = i + 64
        # print(i)
        avg_power = compute_power(sig_in[slide_start:slide_end])
        if(avg_power > max_val):
            max_val = avg_power
            max_idx = i+32

    # print(max_idx)
    slide_start = max_idx - 32
    slide_end = max_idx + 32


    avg_power = compute_power(sig_in[slide_start:slide_end])
    threshold = 0.125 * avg_power


    for i in np.arange(max_idx, n-32, 1):
      slide_start = i - 32
      slide_end = i + 32
      slide_power = compute_power(sig_in[slide_start:slide_end])
      # print(slide_power)
      if slide_power < threshold:
        end = slide_end
        break


    for i in np.arange(32, max_idx, 1):
        i = max_idx-i-1
        slide_start = i-32
        slide_end = i+32

        slide_power = compute_power(sig_in[slide_start:slide_end])

        if slide_power < threshold:
            start = slide_start
            break

    return  start, end


def frame_to_mfcc(frame):

    ##### write your code start #####
    # input frame has 512 samples, extract MFCC from this frame
    frame = np.array(frame).flatten()
    spectrum = librosa.feature.melspectrogram(y = frame,sr = 8000, n_fft = 512,
                                             window = signal.windows.hamming,
                                             center= False, n_mels = 20, htk = True)
    mfcc = librosa.feature.mfcc(S = librosa.power_to_db(spectrum,top_db = None),
                               sr= 8000, n_mfcc = 13, n_fft = 512,hop_length = 128,
                               n_mels = 20, fmax = 4000, htk = True, center = False)
    return mfcc

def mfccfor3(sig):

  f_1 = sig[:512]
  m_1 = frame_to_mfcc(f_1)

  f_2 = sig[384:896]
  m_2 = frame_to_mfcc(f_2)

  f_3 = sig[768:1280]
  m_3 = frame_to_mfcc(f_3)

  all_m = np.concatenate((m_1, m_2, m_3))

  return all_m



def extractmfcc(chord,tone_files):
  tone_list = []

  for i in range(len(tone_files[chord])):

  # for i in range(5):
    name = "./trying8kv10/"+str(int(tones[chord]))+"/"+tone_files.iloc[i,chord]
    # print(name)
    filepath = os.path.join(name)
    waveform = pd.read_csv(filepath).astype(float).to_numpy().flatten()[:32000]
    norm_waveform = normalize_signal(waveform)

    #average of all data
    # norm_waveform = normalize_signal_d(waveform,14450.295838740545,2225.3750311333583) #average of all data
    ##weighted avg of all data(w of 10 = 2, etc = 1) v1
    # norm_waveform = normalize_signal_d(waveform,14524.59360321549,2225.3750311333583)
    #normalized with weigthed average of 7 = 1, etc = 2 v2
    # norm_waveform = normalize_signal_d(waveform, 14441.919043267624,2225.3750311333583)

    ##normalized with weighted average of tone 7 = 2, etc =1 v3
    # norm_waveform = normalize_signal_d(waveform,14465.116323038797,2225.3750311333583)

    #range (might be): 14450 - 14465
    ##normalized with weighted average, weight of 1,4,5,6,7,9,10 = 2, etc = 1
    # norm_waveform = normalize_signal_d(waveform,14457.550577013462,2225.3750311333583)


    ### NEW EXPERIMENT
    ###https://neerajkumar.org/writings/svm/#:~:text=Some%20libraries%20recommend%20doing%20a,(again%2C%20by%20dimension).
    # norm_waveform = normalize_signal_d(waveform,np.mean(waveform), 2 * np.std(waveform))

    tone_list.append(norm_waveform)

  tones_signals = pd.DataFrame(np.transpose(tone_list))
  voice_signal = tones_signals[0]
  s, e = remove_silence(voice_signal)
  voice_signal = voice_signal[s:e]
  curr = 0
  last = len(voice_signal)

  #extract 1280
  allmfcc = mfccfor3(voice_signal[:1280])

  curr = curr+1280-128
  loop = 1
  while curr+1280<last:
    mfcc = mfccfor3(voice_signal[curr:curr+1280])
    allmfcc = np.concatenate((allmfcc,mfcc),axis = 1)
    curr = curr+1280-128
    loop += 1
  print(loop)

  for i in range(1,len(tones_signals.columns)):
    voice_signal = tones_signals[i]
    start,end = remove_silence(voice_signal)
    voice_signal = voice_signal[start:end]
    curr = 0
    last = len(voice_signal)

    mfcc = mfccfor3(voice_signal[:1280])

    allmfcc = np.concatenate((allmfcc,mfcc),axis = 1)
    curr = curr+1280-128
    while curr+1280<last:
      mfcc = mfccfor3(voice_signal[curr:curr+1280])
      allmfcc = np.concatenate((allmfcc,mfcc),axis = 1)
      curr = curr+1280-128


  #   #extract 512
  # allmfcc = frame_to_mfcc(voice_signal[:512])

  # curr = curr+512-128
  # while curr+512<last:
  #   mfcc = frame_to_mfcc(voice_signal[curr:curr+512])
  #   allmfcc = np.concatenate((allmfcc,mfcc),axis = 1)
  #   curr = curr+512-128

  # for i in range(1,len(tones_signals.columns)):
  #   voice_signal = tones_signals[i]
  #   start,end = remove_silence(voice_signal)
  #   voice_signal = voice_signal[start:end]
  #   curr = 0
  #   last = len(voice_signal)

  #   mfcc = frame_to_mfcc(voice_signal[:512])

  #   allmfcc = np.concatenate((allmfcc,mfcc),axis = 1)
  #   curr = curr+512-128
  #   while curr+512<last:
  #     mfcc = frame_to_mfcc(voice_signal[curr:curr+512])
  #     allmfcc = np.concatenate((allmfcc,mfcc),axis = 1)
  #     curr = curr+512-128




  allmfcc = pd.DataFrame(allmfcc)
  print(allmfcc.shape)
  return allmfcc


def rescale(sig):
  MAX_v = 0
  if(max(sig) > abs(min(sig))):
    MAX_v = max(sig)
  else :
    MAX_v = abs(min(sig))
  for i in range(len(sig)):
    sig[i] = (sig[i])  * 1.0/MAX_v
  return sig

"""### MFCC Features extraction from raw datasets sampled at 4000 Hz."""

name = "./trying8kv8/"+str(int(tones[8]))+"/1.txt"
    # print(name)
filepath = os.path.join(name)
waveform = pd.read_csv(filepath).astype(float).to_numpy().flatten()[:32000]
norm_waveform = normalize_signal(waveform)
plt.plot(waveform)
Audio(waveform.reshape(1,-1),rate = 8000)

alltones = np.transpose(np.array(get_tones(0)))

for i in range(1,12):
  tone = np.transpose(np.array(get_tones(i)))
  alltones = np.vstack((alltones,tone))



alltones = pd.DataFrame(np.transpose(alltones))


all_mfcc = {}
for i in range(0,12):
  all_mfcc[i] = pd.DataFrame(extractmfcc(i,alltones))



all_mfcc_down = {}
for i in range(0,12):
  r,c = all_mfcc[i].shape
  choose = random.randint(c)
  mfcc_down = all_mfcc[i][choose].to_numpy()
  # print(type(mfcc_down))
  mfcc_down = mfcc_down.reshape(39,1)
  # for j in range(1,c):
  for j in range(1,250):
    choose = random.randint(c)
    newmfcc = all_mfcc[i][choose].to_numpy()
    mfcc_down = np.concatenate((mfcc_down,newmfcc.reshape(39,1)),axis = 1)
  all_mfcc_down[i] = pd.DataFrame(mfcc_down)
  print(all_mfcc_down[i].shape)

# print(all_mfcc_down[0])

train_mfcc = {}
test_mfcc = {}
for i in range(0,12) :
  mfcc_train , mfcc_test = train_test_split(np.transpose(all_mfcc_down[i]), test_size = 0.2)
  mfcc_train["label"] = i;

  mfcc_test["label"] = i;
  train_mfcc[i] = mfcc_train
  test_mfcc[i] = mfcc_test
  print(i)
  print(train_mfcc[i].shape)
  print(test_mfcc[i].shape)

# print(train_mfcc)

df_train = pd.concat([train_mfcc[0],train_mfcc[1]])
for i in range(2,12):
  chord = tones[i]
  df_train = pd.concat([df_train,train_mfcc[i]])



df_test = pd.concat([test_mfcc[0],test_mfcc[1]])
# print(df_train)
for i in range(2,12):
  chord = tones[i]
  df_test = pd.concat([df_test,test_mfcc[i]])
# print(df_test)


df_train = df_train.sample(frac = 1)

# df_train = pd.concat([df_tone1_train, df_tone2_train, df_tone3_train, df_tone4_train, df_tone5_train])
# df_test = pd.concat([df_tone1_test, df_tone2_test, df_tone3_test, df_tone4_test, df_tone5_test])

# df_train = df_train.sample(frac=1)  # shuffle the data

X_train = df_train.iloc[:,0:df_train.shape[1]-1]
y_train = df_train['label']

X_test = df_test.iloc[:,0:df_train.shape[1]-1]
y_test = df_test['label']


print("X_train shape: ", X_train.shape)
print("y_train shape: ", y_train.shape)
print("X_test shape: ", X_test.shape)
print("y_test shape: ", y_test.shape)

# nomalize features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("max value for each dimension of MFCC", scaler.data_max_)
print("min value for each dimension of MFCC", scaler.data_min_)

"""### Train a SVM model"""

# build your own multi-class SVM
clf = SVC(decision_function_shape='ovo', kernel='rbf', C=0.1, gamma=0.1)
clf.fit(X_train_scaled, y_train)
print(clf)
y_pred_train = clf.predict(X_train_scaled)
y_pred_test = clf.predict(X_test_scaled)

print("testing accuracy score", accuracy_score(y_pred_train, y_train))
print("testing accuracy score", accuracy_score(y_pred_test, y_test))

# ##### write your code start #####

# # use grid search to find best hyper-parameters for your SVM
# # you should use fixed decision_function_shape='ovo', kernel='rbf'
# # and optimize other hyper-parameters (C and gamma)

param_grid = {'C': [0.1,1 ,10,100,1000],
              'gamma': [100,10, 1, 0.1,0.01],
              'kernel': ['rbf']}
grid = GridSearchCV(SVC(decision_function_shape = 'ovo', kernel ='rbf'), param_grid)


grid.fit(X_train_scaled, y_train)
print(grid.best_params_)
print(grid.best_estimator_)

grid2 = grid.best_estimator_
y_pred_train2 = grid2.predict(X_train_scaled)
y_pred_test2 = grid2.predict(X_test_scaled)


print("testing accuracy score", accuracy_score(y_pred_train2, y_train))
print("testing accuracy score", accuracy_score(y_pred_test2, y_test))

# ##### write your code end #####

"""### Save SVM parameters"""

def save_svm_params(clf, scaler):

    """H7 board will parse these parameters"""

    f = open("svmmodel.txt", "w")

    # write header
    line = "svm_type c_svc" + " \n"
    f.write(line)
    line = "kernel_type " + clf.kernel + " \n"
    f.write(line)
    line = "gamma " + str(clf.gamma) + " \n"
    f.write(line)
    line = "n_class " + str(len(clf.classes_)) + " \n"
    f.write(line)
    line = "total_n_sv " + str(np.sum(clf.n_support_)) + " \n"
    f.write(line)
    line = "intercept " + " ".join([str(round(i,8)) for i in clf.intercept_]) + " \n"
    f.write(line)
    line = "label " + " ".join([str(i) for i in clf.classes_]) + " \n"
    f.write(line)
    line = "n_sv " + " ".join([str(i) for i in clf.n_support_]) + " \n"
    f.write(line)
    line = "n_feature " + str(clf.support_vectors_.shape[1]) + " \n"
    f.write(line)
    line = "scaler_min " + " ".join([str(i) for i in scaler.data_min_])  + " \n"
    f.write(line)
    line = "scaler_max " + " ".join([str(i) for i in scaler.data_max_])  + " \n"
    f.write(line)

    # write support vector
    m, n = clf.support_vectors_.shape
    print("sv shape:", m, n)
    line = "SV \n"
    f.write(line)
    for i in range(m):
        line = " ".join([str(i) for i in clf.support_vectors_[i]]) + " \n"
        f.write(line)

    # write dual_coef
    m, n = clf.dual_coef_.shape
    print("dual_coef shape:", m, n)
    line = "dual_coef \n "
    f.write(line)
    for i in range(n):
        line = " ".join([str(i) for i in clf.dual_coef_[:, i]]) + " \n"
        f.write(line)

    f.close()

# save to disk
save_svm_params(grid2, scaler)

"""# SVM Predict and testing



"""

# use the parameters of the SVM model trained in PART1

n_sv = np.sum(grid2.n_support_)
support_vectors = grid2.support_vectors_
n_class = len(grid2.classes_)
gamma = grid2.gamma
def rbf_kernel(x1, x2, gamma):
    m = len(x1)
    acc = 0
    for i in range(m):
        tmp = x1[i] - x2[i]
        acc = acc+ tmp*tmp
    return np.exp(-gamma * acc)

def svm_predict(x):
    """x: MFCC feature in 39 dimensions
       output: tone number
    """
    ##### write your code start #####
    kvalue = np.zeros(n_sv)
    for i in range(n_sv):
      kvalue[i] = rbf_kernel(x, support_vectors[i], gamma)
    start = np.zeros(n_class)
    for i in range(1, n_class):
      start[i] = start[i - 1] + grid2.n_support_[i-1]
    vote = np.zeros(n_class)
    p=0
    for i in range(n_class):
      acc_val = np.zeros(n_class - i - 1)
      for j in range(i + 1, n_class):
        acc = 0
        si = int(start[i])
        sj = int(start[j])
        ci = grid2.n_support_[i]
        cj = grid2.n_support_[j]
        coef1 = grid2.dual_coef_[j - 1]
        coef2 = grid2.dual_coef_[i]
        # if(i == 5 and j == n_class - 1):
        #     print("coef1", "\n")
        for k in range(ci):
          acc = acc + coef1[si + k] * kvalue[si + k]
        #   if(i == 5 and j == n_class - 1):
        #     print(acc,"\n")
        # if(i == 5 and j == n_class - 1):
        #     print("coef2", "\n")
        for k in range(cj):
          acc = acc + coef2[sj + k] * kvalue[sj + k]
          # if(i == 5 and j == n_class - 1):
          #   print(acc,"\n")
        acc = acc + grid2.intercept_[p]
        # if(i == 5 and j == n_class - 1):
        #   print("intercept \n")
        #   print(acc,"\n")
        acc_val[j-i-1] = acc
        if(acc > 0):
          vote[i] = vote[i] + 1
        else:
          vote[j] = vote[j] + 1
        p = p + 1
    #   ipdb.set_trace(context = 6)
    # ipdb.set_trace(context = 6)
    vote_max_idx = 0
    # print(vote)
    for i in range(n_class):
      if(vote[i] > vote[vote_max_idx]):
        vote_max_idx = i
    return vote_max_idx
    ##### write your code end #####

name = "./retest/"+str(tones[9])+"/"+"10.txt"
# print(name)
filepath = os.path.join(name)
waveform = pd.read_csv(filepath).astype(float).to_numpy().flatten()[:16000]
print(waveform[26])
print(waveform[608])
print(waveform[15998])


waveform = normalize_signal(waveform)
print(waveform[608])
# s , e = remove_silence(waveform)
# waveform = waveform[s:e]
# print(s)
# print(e)
# print(len(waveform))

def hammingWindow(size,real_buff):
  hamming = np.zeros(size)

  for i in range(size):
    hamming[i] = float(0.540000000000 - 0.460000000000 * math.cos(2.0000000000 * (22.00000000000/7.00000000000) * i / float(size - 1.0000000000)))

  for i in range(size):
    real_buff[i] = float(real_buff[i] * hamming[i])

  return real_buff

def convertToComplex(size, real_buff):
  complex_buff = np.zeros(2 * size)
  for i in range(size):
    complex_buff[2 * i] = real_buff[i]
  return complex_buff

def powerSpectrum(size, complex_buff):
  real_buff = np.zeros(int(size/2 + 1))
  for i in range(int(size/2 + 1)):
    real_buff[i] = pow(abs(complex_buff[i].real),2) + pow(abs(complex_buff[i].imag),2)
  return real_buff

def freqToMel(freq):
  return 1127.01048 * math.log(1 + freq / 700.0)

def melToFreq(mel):
  return 700.0 * (math.exp(mel / 1127.01048) - 1)

def filteredSpectrum(sr,n_fft,n_mels,freq_min,freq_max,power_spec_buff):
  fmin_mel,fmax_mel,mel_step,linear_freq_step,inner_sum = 0,0,0,0,0

  filter = np.zeros(int(n_fft/2 + 1))
  mels = np.zeros(n_mels + 2)
  mels_f = np.zeros(n_mels + 2)
  linar_freq = np.zeros(int(n_fft/2+1))
  upper = np.zeros(int(n_fft/2+1))
  lower = np.zeros(int(n_fft/2+1))
  enorm = np.zeros(n_mels)

  filtered_spec = np.zeros(n_mels)

  fmin_mel = freqToMel(freq_min)
  fmax_mel = freqToMel(freq_max)

  linear_freq_step = (sr/2.0 - freq_min)/(n_fft/2)
  for i in range(int(n_fft/2 + 1)):
    linar_freq[i] = i * linear_freq_step

  mel_step = (fmax_mel - fmin_mel)/ (n_mels + 1)
  for i in range(n_mels + 2):
      mels_f[i] = melToFreq(i * mel_step)
  for i in range(n_mels):
    enorm[i] = 2.0 / (mels_f[i+2] - mels_f[i])

  for jth_filter in range(n_mels):
    for i in range(int(n_fft/2+1)):
      lower[i] = (linar_freq[i] - mels_f[jth_filter]) / (mels_f[jth_filter+1]-mels_f[jth_filter])

    for i in range(int(n_fft/2+1)):
      upper[i] = (mels_f[jth_filter+2] - linar_freq[i])/ (mels_f[jth_filter+2]-mels_f[jth_filter+1])

    # compute filter matrix
    for i in range(int(n_fft/2+1)):
      filter[i] = max(0, min(upper[i], lower[i]))


		#  normalize filter matrix
    for i in range(int(n_fft/2+1)):
      filter[i] *= enorm[jth_filter]

  #  compute filtered spectrum
    inner_sum = 0.0

    for i in range(int(n_fft/2+1)):
      if filter[i] == 0.0:
        continue        #skip zero multiplication
      else:
        inner_sum = inner_sum + filter[i] * power_spec_buff[i]


    filtered_spec[jth_filter] = inner_sum
  return filtered_spec

def powerToDB(size, real_buff):
  new_real_buff = np.zeros(size)
  for i in range(size):
    new_real_buff[i] = 10 * math.log10(real_buff[i])
  return new_real_buff



def dctTransform(size,buff_in):
  acc, scale = 0.0,0.0
  buff_out = np.zeros(size)
  for i in range(size):
    acc = 0.0
    for j in range(size):
      acc += buff_in[j] * math.cos(math.pi * (j + 0.5) * i / size)

    if i == 0:
      scale = math.sqrt(1.0/(4.0 * size)) * 2
    else:
      scale = math.sqrt(1.0/(2.0 *size)) * 2

    buff_out[i] = acc * scale

  return buff_out

def norm_sig(sig):
    m_val = 0
    s_val = 0
    for i in range(len(sig)):
      m_val += sig[i]
    m_val /= len(sig)

    for i in range(len(sig)):
      s_val += math.pow(sig[i] - m_val,2.0)
    s_val = math.sqrt(s_val/len(sig))

    for i in range(len(sig)):
      sig[i] = (sig[i] - m_val)/ s_val


    print(m_val)
    print(s_val)

    return sig

def normalizeFeature(feature):
  for i in range(len(feature)):
    feature[i] = (feature[i] - scaler.data_min_[i])/(scaler.data_max_[i]-scaler.data_min_[i])
  return feature


def reScale(sig):
  MAX_v = max(sig)
  MIN_v = min(sig)
  for i in range(len(sig)):
    sig[i] = (sig[i] - MIN_v)  * 32767.0/ (MAX_v - MIN_v)
  return sig


def remove_outlier(sig):
  sig_mean = np.mean(sig)
  for i in range(len(sig)):
    if (abs(waveform[i] - sig_mean) > 0.6 * sig_mean):
      sig[i] = sig_mean
  return sig



!pip install ffmpeg-python

"""
To write this piece of code I took inspiration/code from a lot of places.
It was late night, so I'm not sure how much I created or just copied o.O
Here are some of the possible references:
https://blog.addpipe.com/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript/
https://stackoverflow.com/a/18650249
https://hacks.mozilla.org/2014/06/easy-audio-capture-with-the-mediarecorder-api/
https://air.ghost.io/recording-to-an-audio-file-using-html5-and-js/
https://stackoverflow.com/a/49019356
"""
from IPython.display import HTML, Audio
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from scipy.io.wavfile import read as wav_read
import io
import ffmpeg

AUDIO_HTML = """
<script>
var my_div = document.createElement("DIV");
var my_p = document.createElement("P");
var my_btn = document.createElement("BUTTON");
var t = document.createTextNode("Press to start recording");

my_btn.appendChild(t);
//my_p.appendChild(my_btn);
my_div.appendChild(my_btn);
document.body.appendChild(my_div);

var base64data = 0;
var reader;
var recorder, gumStream;
var recordButton = my_btn;

var handleSuccess = function(stream) {
  gumStream = stream;
  var options = {
    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k
    mimeType : 'audio/webm;codecs=opus'
    //mimeType : 'audio/webm;codecs=pcm'
  };
  //recorder = new MediaRecorder(stream, options);
  recorder = new MediaRecorder(stream);
  recorder.ondataavailable = function(e) {
    var url = URL.createObjectURL(e.data);
    var preview = document.createElement('audio');
    preview.controls = true;
    preview.src = url;
    document.body.appendChild(preview);

    reader = new FileReader();
    reader.readAsDataURL(e.data);
    reader.onloadend = function() {
      base64data = reader.result;
      //console.log("Inside FileReader:" + base64data);
    }
  };
  recorder.start();
  };

recordButton.innerText = "Recording... press to stop";

navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);


function toggleRecording() {
  if (recorder && recorder.state == "recording") {
      recorder.stop();
      gumStream.getAudioTracks()[0].stop();
      recordButton.innerText = "Saving the recording... pls wait!"
  }
}

// https://stackoverflow.com/a/951057
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

var data = new Promise(resolve=>{
//recordButton.addEventListener("click", toggleRecording);
recordButton.onclick = ()=>{
toggleRecording()

sleep(2000).then(() => {
  // wait 2000ms for the data to be available...
  // ideally this should use something like await...
  //console.log("Inside data:" + base64data)
  resolve(base64data.toString())

});

}
});

</script>
"""

def get_audio():
  display(HTML(AUDIO_HTML))
  data = eval_js("data")
  binary = b64decode(data.split(',')[1])

  process = (ffmpeg
    .input('pipe:0')
    .output('pipe:1', format='wav')
    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)
  )
  output, err = process.communicate(input=binary)

  riff_chunk_size = len(output) - 8
  # Break up the chunk size into four bytes, held in b.
  q = riff_chunk_size
  b = []
  for i in range(4):
      q, r = divmod(q, 256)
      b.append(r)

  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.
  riff = output[:4] + bytes(b) + output[8:]

  sr, audio = wav_read(io.BytesIO(riff))

  return audio, sr

audio, sr = get_audio()

waveform = audio
plt.plot(waveform)

# from scipy import fft
#YEET YEET YEET YEET YEET
sample_rate = 4000
n_mels = 20
freq_min = 0
freq_max = 2000
overlap_size = 128
FFTSize = 512
n_mfcc = 13
# n_frames = 1 + (len(waveform)- FFTSize) / overlap_size
n_frames = 0



res = 0


vote_res = np.zeros(12)
# mfcc_feature = np.zeros(3 * n_mfcc)
mfcc_feature = np.zeros(n_mfcc)
frame = np.zeros(FFTSize)

allmfcc = np.zeros(n_mfcc)

# DATA DATA DATA DATA DATA DATA
# name = "./retestv2/"+str(tones[0])+"/"+"10.txt"
# print(name)
# filepath = os.path.join(name)
# waveform = pd.read_csv(filepath).astype(float).to_numpy().flatten()[:16000]

#####LIVE AUDIO TESTING
waveform = signal.decimate(audio, 12)
# waveform = remove_outlier(waveform)
wf = waveform
# print("Mean value : " ,np.mean(waveform))
# print("Standard deviation value : " ,np.std(waveform))
# print("Using the for loop")
# m_val = 0
# s_val = 0
# for i in range(len(waveform)):
#   m_val += waveform[i]
# m_val /= len(waveform)

# for i in range(len(waveform)):
#   s_val += math.pow(waveform[i] - m_val,2.0)
# s_val = math.sqrt(s_val/len(waveform))
# print("Mean value (loop): " , m_val)
# print("Standard Deviation value (loop) : ", s_val)
waveform = normalize_signal(waveform)
# waveform = normalize_signal_d(waveform,-0.045349246364934835,1390.8013549285922)
# waveform = normalize_signal_d(waveform,np.mean(waveform), 2 * np.std(waveform))
# waveform = normalize_signal_d(waveform,14450.295838740545,2225.3750311333583) #Average of all the data
# waveform = normalize_signal_d(waveform,14524.59360321549,2225.3750311333583) #weighted average, weight of tone 10 = 2, etc = 1
#normalized with weighted average of weight tone 7 = 1, etc = 2
# waveform = normalize_signal_d(waveform,14441.919043267624,2225.3750311333583)
#normalized with weighted average of weight tone 7 = 2, etc = 1
# waveform = normalize_signal_d(waveform, 14465.116323038797,2225.3750311333583)
# plt.plot(waveform)
# print(waveform)
w_test = norm_sig(wf)

# print(waveform)
s , e = remove_silence(waveform)
waveform = waveform[s:e]
# print(s)
# print(e)
s_o = len(waveform)
# print(s_o)
while(s_o - FFTSize + 128 > 0):
  s_o -= FFTSize - 128
  n_frames += 1
# print(s_o)
n_res = math.floor(n_frames / 3) - 1
n_frames -= 1
# DATA DATA DATA DATA DATA DATA
v_res = np.zeros(12)
t_4 = 0
t_3 = 0
t_5 = 0
t_6 = 0
# print(grid2.intercept_)
# print(len(grid2.intercept_))




frame = waveform[0:512]


d_1 = np.hamming(FFTSize)
demo1 = np.zeros(FFTSize)
for k in range(FFTSize):
  demo1[k] = frame[k] * d_1[k]
demo2 = fft(demo1)
demo3 = powerSpectrum(FFTSize, demo2)
demo4 = filteredSpectrum(sample_rate, FFTSize, n_mels, freq_min, freq_max,demo3)
demo5 = powerToDB(n_mels,demo4)
demo6 = dctTransform(n_mels,demo5)
for j in range(n_mfcc):
  mfcc_feature[j] = demo6[j]
allmfcc = mfcc_feature
result = svm_predict(normalizeFeature(mfcc_feature))
vote_res[result] += 1
allmfcc = allmfcc.reshape(-1, 1)
for i in range(1,n_frames):
  frame = waveform[i*(FFTSize - overlap_size):i*(FFTSize - overlap_size) + 512]

  d_1 = np.hamming(FFTSize)
  demo1 = np.zeros(FFTSize)
  for k in range(FFTSize):
    demo1[k] = frame[k] * d_1[k]
  demo2 = fft(demo1)
  demo3 = powerSpectrum(FFTSize, demo2)
  demo4 = filteredSpectrum(sample_rate, FFTSize, n_mels, freq_min, freq_max,demo3)
  demo5 = powerToDB(n_mels,demo4)
  demo6 = dctTransform(n_mels,demo5)

  for j in range(n_mfcc):
    mfcc_feature[j] = demo6[j]
  allmfcc = np.concatenate((allmfcc,mfcc_feature.reshape(-1, 1)),axis = 1)
  result = svm_predict(normalizeFeature(mfcc_feature))
  vote_res[result] += 1



#####BELOW THIS IS THE ORIGINAL ONE , 39 FEATURES


# for i in range(3):
#     frame = waveform[i*(FFTSize - overlap_size):i*(FFTSize - overlap_size) + 512]


#     d_1 = np.hamming(FFTSize)
#     demo1 = np.zeros(FFTSize)
#     for k in range(FFTSize):
#       demo1[k] = frame[k] * d_1[k]
#     # demo1 = hammingWindow(FFTSize,frame)
#     # ipdb.set_trace(context = 6)
#     demo2 = fft(demo1)
#     # ipdb.set_trace(context = 6)
#     demo3 = powerSpectrum(FFTSize, demo2)
#     demo4 = filteredSpectrum(sample_rate, FFTSize, n_mels, freq_min, freq_max,demo3)
#     demo5 = powerToDB(n_mels,demo4)
#     demo6 = dctTransform(n_mels,demo5)
#     # print('one')
#     # print(demo1)
#     # print('two')
#     # print(demo2)
#     # print('three')
#     # print(demo3)
#     # print('four')
#     # print(demo4)
#     # print('five')
#     # print(demo5)
#     # print('six')
#     # print(demo6)
#     # sys.exit()
#     # DEMO PURPOSES ONLY
#     # ipdb.set_trace(context = 6)
#     for j in range(n_mfcc):
#       # mfcc_feature[i * n_mfcc + j] = frame6[j]
#       mfcc_feature[i*n_mfcc+j] = demo6[j]
# allmfcc = mfcc_feature
# # ipdb.set_trace(context = 6)
# result = svm_predict(normalizeFeature(mfcc_feature))
# vote_res[result] += 1
# # print(result+1)
# allmfcc = allmfcc.reshape(-1, 1)
# # ipdb.set_trace(context = 6)
# for k in range(1,n_res):
#   mfcc_feature = np.zeros(3 * n_mfcc)
#   for i in range(3):
#     index = k * (1280 - overlap_size) + i *(FFTSize - overlap_size)
#     frame = waveform[index:index+ 512]
#     d_1 = np.hamming(FFTSize)
#     demo1 = np.zeros(FFTSize)
#     for p in range(FFTSize):
#       demo1[p] = frame[p] * d_1[p]
#     demo2 = fft(demo1)

#     demo3 = powerSpectrum(FFTSize, demo2)
#     demo4 = filteredSpectrum(sample_rate, FFTSize, n_mels, freq_min, freq_max,demo3)
#     demo5 = powerToDB(n_mels,demo4)
#     demo6 = dctTransform(n_mels,demo5)
#     for j in range(n_mfcc):
#       mfcc_feature[i * n_mfcc + j] = demo6[j]
#   result = svm_predict(normalizeFeature(mfcc_feature))
#   allmfcc = np.concatenate((allmfcc,mfcc_feature.reshape(-1, 1)),axis = 1)

#   vote_res[result] += 1

allmfcc = pd.DataFrame(allmfcc)
# print(allmfcc)
# print(am)
print(vote_res)

print(len(waveform))
print(len(audio))
print(len(wf))

alltones = np.transpose(np.array(get_tones(0)))

for i in range(1,12):
  tone = np.transpose(np.array(get_tones(i)))
  alltones = np.vstack((alltones,tone))



alltones = pd.DataFrame(np.transpose(alltones))

mean_chord = []
std_chord = []
for j in range(12):
  t_mean = 0;
  t_std = 0;
  for i in range(len(alltones[j])):
  # for i in range(100):
    name = "./trying8kv4/"+str(tones[j])+"/"+alltones.iloc[i,j]
    filepath = os.path.join(name)
    waveform = pd.read_csv(filepath).astype(float).to_numpy().flatten()[:32000]
    t_mean += np.mean(waveform)
    t_std += np.std(waveform)
  t_mean /= len(alltones[j])
  t_std /= len(alltones[j])
  mean_chord.append(t_mean)
  std_chord.append(t_std)


print(mean_chord)
print(std_chord)
print(np.mean(mean_chord))
print(np.mean(std_chord))